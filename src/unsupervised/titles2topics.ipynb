{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains an LSI, LDA, and NMF to come up with groupings of terms that should be in the same topics.\n",
    "These algorithms do not give topic names other than topic0 topic1 etc. ChatGPT was asked to name the topics.\n",
    "\n",
    "These algorithms can be used to label a new document with the most likely topics by probability that correspond to it\n",
    "ie: predict_topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Views</th>\n",
       "      <th>Is_Generated</th>\n",
       "      <th>Transcript_Blob</th>\n",
       "      <th>Transcript_Clean</th>\n",
       "      <th>Title_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apple Pay Is Killing the Physical Wallet After...</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>tech</td>\n",
       "      <td>3407.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>135612.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Music] this is your tech news briefing for t...</td>\n",
       "      <td>music tech news brief tuesday august 23rd im z...</td>\n",
       "      <td>appl pay kill physic wallet eight year tech ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The most EXPENSIVE thing I own.</td>\n",
       "      <td>b3x28s61q3c</td>\n",
       "      <td>2022-08-24</td>\n",
       "      <td>tech</td>\n",
       "      <td>76779.0</td>\n",
       "      <td>4306.0</td>\n",
       "      <td>1758063.0</td>\n",
       "      <td>True</td>\n",
       "      <td>this is my car this is my wife's car and this...</td>\n",
       "      <td>car wife car right power suppli tester bought ...</td>\n",
       "      <td>expens thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title     Video ID  \\\n",
       "0           0  Apple Pay Is Killing the Physical Wallet After...  wAZZ-UWGVHI   \n",
       "1           1                    The most EXPENSIVE thing I own.  b3x28s61q3c   \n",
       "\n",
       "  Published At Keyword    Likes  Comments      Views Is_Generated  \\\n",
       "0   2022-08-23    tech   3407.0     672.0   135612.0         True   \n",
       "1   2022-08-24    tech  76779.0    4306.0  1758063.0         True   \n",
       "\n",
       "                                     Transcript_Blob  \\\n",
       "0   [Music] this is your tech news briefing for t...   \n",
       "1   this is my car this is my wife's car and this...   \n",
       "\n",
       "                                    Transcript_Clean  \\\n",
       "0  music tech news brief tuesday august 23rd im z...   \n",
       "1  car wife car right power suppli tester bought ...   \n",
       "\n",
       "                                         Title_Clean  \n",
       "0  appl pay kill physic wallet eight year tech ne...  \n",
       "1                                       expens thing  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '/Users/gabrielalon/Desktop/Milestone2/siads_696_milestoneII_project/src/data_cleaning/data/video_transcripts_clean.csv'\n",
    "path = '../data_cleaning/data/video_transcripts_clean.csv'\n",
    "\n",
    "clean_transcripts = pd.read_csv(path)\n",
    "clean_transcripts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n"
     ]
    }
   ],
   "source": [
    "clean_transcripts_list = clean_transcripts['Title'].tolist()\n",
    "documents_train = clean_transcripts_list\n",
    "print(len(documents_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 1), min_df=2, max_df=0.95, stop_words=\"english\"\n",
    ")  # default English stopwords\n",
    "\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "n_topics = 5\n",
    "lsi = TruncatedSVD(n_components=n_topics, random_state=0)\n",
    "\n",
    "# This is the matrix U_k:  num_term_features x num_topics\n",
    "reduced_term_matrix = lsi.fit_transform(np.transpose(tfidf_documents))\n",
    "\n",
    "# and this is the matrix V_k^T  num_topics x num_documents\n",
    "reduced_document_matrix = lsi.components_\n",
    "\n",
    "# these are the the values along the diagonal of matrix \\Sigma.\n",
    "singular_values = lsi.singular_values_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#In Progress Applying LSI to label videos as topics\n",
    "    # Missing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881, 4590)\n",
      "topic 0: ['39', 'amp', 'mukbang', 'cube', 'biology', 'rubik', 'asmr', 'eating']\n",
      "topic 1: ['xbox', '39', '000', 'video', '100', 'vs', 'animals', 'trolling']\n",
      "topic 2: ['apple', 'iphone', '2022', '14', '39', 'new', 'amp', 'games']\n",
      "topic 3: ['quot', 'sat', 'literature', 'reaction', 'review', 'score', 'bed', 'movie']\n",
      "topic 4: ['game', '2022', '39', 'development', 'amp', 'questions', 'new', 'answers']\n",
      "topic 5: ['nintendo', 'switch', 'business', 'interview', 'food', 'job', 'movies', 'best']\n",
      "topic 6: ['lofi', 'music', 'sports', 'hip', 'hop', 'chill', 'beats', 'study']\n",
      "topic 7: ['science', 'learning', 'data', 'machine', 'computer', 'course', 'beginners', 'tutorial']\n",
      "topic 8: ['2022', 'best', 'music', 'songs', 'marvel', 'food', 'studios', 'gaming']\n",
      "topic 9: ['sat', 'world', '39', 'things', 'google', 'trolling', 'tips', 'study']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "\n",
    "# get_feature_names() deprecated\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "tf_documents = tf_vectorizer.fit_transform(documents_train)\n",
    "\n",
    "# get_feature_names() deprecated\n",
    "# tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(tf_documents.shape)\n",
    "\n",
    "# This cell will take a couple of minutes to run...\n",
    "n_topics = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_\n",
    "\n",
    "num_top_words = 8\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        term_list = [\n",
    "            feature_names[i] for i in topic.argsort()[: -no_top_words - 1 : -1]\n",
    "        ]\n",
    "        print(\"topic %d:\" % (topic_idx), term_list)\n",
    "\n",
    "display_topics(lda, tf_feature_names, num_top_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LDA to  Predict Topics on a New Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top topics for 'My nintendo switch':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 9, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = \"My nintendo switch\"\n",
    "print(f\"The Top topics for '{new_document}':\")\n",
    "def predict_topics_lda(doc, num_topics=3):\n",
    "    new_document_vector = tf_vectorizer.transform([new_document])\n",
    "    topic_probabilities = lda.transform(new_document_vector)\n",
    "   # print(topic_probabilities)\n",
    "    flat_values = np.ravel(topic_probabilities)\n",
    "    descending_indexes = np.argsort(flat_values)[::-1]\n",
    "    top_n_topics_sorted = descending_indexes[:num_topics]\n",
    "    #print(descending_indexes[0])\n",
    "    return list(top_n_topics_sorted)\n",
    "predict_topics_lda(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([\"ASMR and Rubik's Cube Science\", 'Gaming Encounters and Pranks', 'Latest Apple Tech and Games', 'Movie Reviews and Literature', 'Game Development Insights', 'Nintendo Business and Entertainment', 'Lo-fi Music for Study and Sports', 'Intro to Machine Learning', 'Best of 2022: Music, Movies, and Gaming', 'Worldly Study Tips and Tricks'])\n"
     ]
    }
   ],
   "source": [
    "#CHATGPT\n",
    "topic_dictionary = {\n",
    "    \"ASMR and Rubik's Cube Science\": ['39', 'amp', 'mukbang', 'cube', 'biology', 'rubik', 'asmr', 'eating'],\n",
    "    \"Gaming Encounters and Pranks\": ['xbox', '39', '000', 'video', '100', 'vs', 'animals', 'trolling'],\n",
    "    \"Latest Apple Tech and Games\": ['apple', 'iphone', '2022', '14', '39', 'new', 'amp', 'games'],\n",
    "    \"Movie Reviews and Literature\": ['quot', 'sat', 'literature', 'reaction', 'review', 'score', 'bed', 'movie'],\n",
    "    \"Game Development Insights\": ['game', '2022', '39', 'development', 'amp', 'questions', 'new', 'answers'],\n",
    "    \"Nintendo Business and Entertainment\": ['nintendo', 'switch', 'business', 'interview', 'food', 'job', 'movies', 'best'],\n",
    "    \"Lo-fi Music for Study and Sports\": ['lofi', 'music', 'sports', 'hip', 'hop', 'chill', 'beats', 'study'],\n",
    "    \"Intro to Machine Learning\": ['science', 'learning', 'data', 'machine', 'computer', 'course', 'beginners', 'tutorial'],\n",
    "    \"Best of 2022: Music, Movies, and Gaming\": ['2022', 'best', 'music', 'songs', 'marvel', 'food', 'studios', 'gaming'],\n",
    "    \"Worldly Study Tips and Tricks\": ['sat', 'world', '39', 'things', 'google', 'trolling', 'tips', 'study']\n",
    "}\n",
    "print(topic_dictionary.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest topic 0 weight is document 857 : ASMR MUKBANG 편의점 핵불닭\n",
      "Highest topic 1 weight is document 1668 : THE ROCK quiere DC V\n",
      "Highest topic 2 weight is document 1612 : LEAKED Gameplay for \n",
      "Highest topic 3 weight is document 639 : NLE Choppa on Beef w\n",
      "Highest topic 4 weight is document 1201 : MÌNH SINH TỒN 100 NG\n",
      "Highest topic 5 weight is document 588 : ZOMBIE GIRL ESCAPE P\n",
      "Highest topic 6 weight is document 1112 : Chạnh Lòng Thương Cô\n",
      "Highest topic 7 weight is document 174 : Sports Car Giant Tes\n",
      "Highest topic 8 weight is document 66 : Breaking News: Jhark\n",
      "Highest topic 9 weight is document 95 : Free Fire | Bất Ngờ \n"
     ]
    }
   ],
   "source": [
    "# Use transform on the original document-term matrix to get the document weights per topic\n",
    "\n",
    "lda_output = lda.transform(tf_documents)\n",
    "\n",
    "#print(\"LDA transform output:\\n\", lda_output)\n",
    "\n",
    "best_document_per_topic = np.argsort(lda_output, axis=0)[::-1]\n",
    "for topic_index in range(0, 10):\n",
    "    best_index = best_document_per_topic[0, topic_index]\n",
    "    print(\n",
    "        \"Highest topic\",\n",
    "        topic_index,\n",
    "        \"weight is document\",\n",
    "        best_index,\n",
    "        \":\",\n",
    "        documents_train[best_index][0:20],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0 ['science', 'data', 'computer', 'course', 'learn', 'beginners', 'scientist', 'minutes']\n",
      "topic  1 ['learning', 'machine', 'tutorial', 'learn', 'python', 'beginners', 'course', 'deep']\n",
      "topic  2 ['lofi', 'hip', 'hop', 'chill', 'beats', 'study', 'music', 'mix']\n",
      "topic  3 ['39', 'cube', 'rubik', 'solve', 'world', 'google', 'apple', 'cubes']\n",
      "topic  4 ['2022', 'movies', 'best', 'action', 'new', 'songs', 'edm', 'music']\n",
      "topic  5 ['game', 'development', 'years', 'unity', 'learning', 'indie', 'developer', 'xbox']\n",
      "topic  6 ['questions', 'answers', 'tech', 'wired', 'twitter', 'support', 'interview', 'biology']\n",
      "topic  7 ['official', 'video', 'music', 'bed', 'marvel', 'studios', 'business', 'disney']\n",
      "topic  8 ['amp', 'asmr', 'mukbang', 'food', 'eating', '먹방', 'noodles', '만든']\n",
      "topic  9 ['quot', 'sports', 'nintendo', 'switch', 'moments', 'history', 'craziest', 'literature']\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "X = tfidf_documents\n",
    "\n",
    "nmf = decomposition.NMF(n_components=n_topics, random_state=0, init=\"nndsvd\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "top = 8\n",
    "topic_index_max = n_topics\n",
    "\n",
    "for topic_index in range(0, topic_index_max):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(tfidf_feature_names[term_index])\n",
    "    print(\"topic \", topic_index, top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introductory Data Science', 'Machine Learning Basics', 'Lo-fi Hip Hop Music', \"Rubik's Cube and Tech Giants\", '2022 Media and Entertainment', 'Game Development Journey', 'Tech & Biology Q&A', 'Music Videos and Business Media', 'ASMR & Mukbang Delights', 'Sports and Literature Moments'])\n"
     ]
    }
   ],
   "source": [
    "#CHATGPT generated\n",
    "topic_dictionary = {\n",
    "    \"Introductory Data Science\": ['science', 'data', 'computer', 'course', 'learn', 'beginners', 'scientist', 'minutes'],\n",
    "    \"Machine Learning Basics\": ['learning', 'machine', 'tutorial', 'learn', 'python', 'beginners', 'course', 'deep'],\n",
    "    \"Lo-fi Hip Hop Music\": ['lofi', 'hip', 'hop', 'chill', 'beats', 'study', 'music', 'mix'],\n",
    "    \"Rubik's Cube and Tech Giants\": ['39', 'cube', 'rubik', 'solve', 'world', 'google', 'apple', 'cubes'],\n",
    "    \"2022 Media and Entertainment\": ['2022', 'movies', 'best', 'action', 'new', 'songs', 'edm', 'music'],\n",
    "    \"Game Development Journey\": ['game', 'development', 'years', 'unity', 'learning', 'indie', 'developer', 'xbox'],\n",
    "    \"Tech & Biology Q&A\": ['questions', 'answers', 'tech', 'wired', 'twitter', 'support', 'interview', 'biology'],\n",
    "    \"Music Videos and Business Media\": ['official', 'video', 'music', 'bed', 'marvel', 'studios', 'business', 'disney'],\n",
    "    \"ASMR & Mukbang Delights\": ['amp', 'asmr', 'mukbang', 'food', 'eating', '먹방', 'noodles', '만든'],\n",
    "    \"Sports and Literature Moments\": ['quot', 'sports', 'nintendo', 'switch', 'moments', 'history', 'craziest', 'literature']\n",
    "}\n",
    "print(topic_dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrained_path = '../../../word2vec/GoogleNews-vectors-negative300.bin'\n",
    "Word2VecModel = KeyedVectors.load_word2vec_format(pretrained_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.40483522, 0.31330317, 0.2883513)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#https://www.kaggle.com/code/nkitgupta/text-representations\n",
    "def topical_coherence(items, w2vmodel):\n",
    "\n",
    "    result = []\n",
    "    for item in items:\n",
    "        try:\n",
    "            if w2vmodel==Word2VecModel:\n",
    "                result.append(w2vmodel[item[0]])\n",
    "            elif w2vmodel==glove_embeddings:\n",
    "                result.append(w2vmodel[item[0]])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(result) == 0:\n",
    "        return 0\n",
    "\n",
    "    matrix_sim = cosine_similarity(result)\n",
    "    np.fill_diagonal(matrix_sim, 0)\n",
    "    return np.mean(matrix_sim)\n",
    "\n",
    "def answer_coherence_a(w2vmodel):\n",
    "    a = topical_coherence(['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport'], w2vmodel)\n",
    "    b = topical_coherence(['scsi', 'drive', 'computer', 'storage', 'megabyte'], w2vmodel)\n",
    "    c = topical_coherence(['introduction', 'pickle', 'guard', 'red', 'valiant'], w2vmodel)\n",
    "\n",
    "    return a, b, c\n",
    "print(answer_coherence_a(w2vmodel=Word2VecModel))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Average Cluster Coherence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29e8d7c574c1c3fa7d750bb72c4ea3c797abd9d323046819b3334e09977e0d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
