{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Doc2Vec\n",
    "import warnings\n",
    "import mplcursors\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import PyQt6\n",
    "import PySide6\n",
    "# import PyQt5\n",
    "# import PySide2\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means is used to find the Nearest Documents and Document Clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After learning K-means, we have videos clustered by similarity according to Doc2Vec.\n",
    "\n",
    "\n",
    "2. We can recommend videos within the closest cluster if that video is queried. \n",
    "\n",
    "-If we get the vector for the new video we can do kmeans.predict but we probably have to retrain the pca\n",
    "\n",
    "-Or, as a weaker substitute we can do a for loop over the k means center videos and do a cosine similarity between a query\n",
    "and those centers to find which cluster is relevant to recommend to the user.\n",
    "\n",
    "3. As an additional step we can find the cosine similarity between a video and a recommended video or its similarity within a large pretrained embedding like GloVE to make sure the recommendations in that cluster are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Views</th>\n",
       "      <th>Is_Generated</th>\n",
       "      <th>Transcript_Blob</th>\n",
       "      <th>Transcript_Clean</th>\n",
       "      <th>Title_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apple Pay Is Killing the Physical Wallet After...</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>tech</td>\n",
       "      <td>3407.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>135612.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Music] this is your tech news briefing for t...</td>\n",
       "      <td>music tech news brief tuesday august 23rd im z...</td>\n",
       "      <td>appl pay kill physic wallet eight year tech ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title     Video ID  \\\n",
       "0           0  Apple Pay Is Killing the Physical Wallet After...  wAZZ-UWGVHI   \n",
       "\n",
       "  Published At Keyword   Likes  Comments     Views Is_Generated  \\\n",
       "0   2022-08-23    tech  3407.0     672.0  135612.0         True   \n",
       "\n",
       "                                     Transcript_Blob  \\\n",
       "0   [Music] this is your tech news briefing for t...   \n",
       "\n",
       "                                    Transcript_Clean  \\\n",
       "0  music tech news brief tuesday august 23rd im z...   \n",
       "\n",
       "                                         Title_Clean  \n",
       "0  appl pay kill physic wallet eight year tech ne...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data_cleaning/data/video_transcripts_clean.csv'\n",
    "clean_transcripts = pd.read_csv(path)\n",
    "clean_transcripts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_transcripts['Title_and_Transcript_Clean'] = clean_transcripts['Title_Clean'] + ' ' + clean_transcripts['Transcript_Clean']\n",
    "clean_titles_list = clean_transcripts['Title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column contains stopwords.\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "has_stopwords = clean_transcripts['Title_and_Transcript_Clean'].apply(lambda x: isinstance(x, str) and any(word.lower() in stop_words for word in x.split()))\n",
    "\n",
    "# Check if any values in the column contain stopwords\n",
    "if has_stopwords.any():\n",
    "    print(\"The column contains stopwords.\")\n",
    "else:\n",
    "    print(\"The column does not contain stopwords.\")\n",
    "\n",
    "\n",
    "print( len(stop_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_transcripts['Transcript_Clean'] = clean_transcripts['Transcript_Clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "#clean_transcripts['Title_Clean'] = clean_transcripts['Title_Clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "#clean_transcripts['Title_and_Transcript_Clean'] = clean_transcripts['Title_and_Transcript_Clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "clean_transcripts['Title_and_Transcript_Clean'] = clean_transcripts['Title_and_Transcript_Clean'].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n",
    "\n",
    "\n",
    "has_stopwords = clean_transcripts['Title_and_Transcript_Clean'].apply(lambda x: isinstance(x, str) and any(word.lower() in stop_words for word in x.split()))\n",
    "\n",
    "# Check if any values in the column contain stopwords\n",
    "if has_stopwords.any():\n",
    "    print(\"The column contains stopwords.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_stopwords = clean_transcripts['Title_and_Transcript_Clean'].apply( lambda x:[word.lower() for word in x.split() if isinstance(x, str) and word.lower() in stop_words])\n",
    "\n",
    "for wordlist in identified_stopwords:\n",
    "    if len(wordlist) > 0:\n",
    "        print( wordlist )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the Data to train on \n",
    "\n",
    "#I suspect that the Title is the most important - but is the transcript better?\n",
    "\n",
    "#all_content_train = clean_transcripts['Transcript_Blob'].astype(str).tolist()\n",
    "#all_content_train = clean_transcripts['Transcript_Clean'].astype(str).tolist()\n",
    "#all_content_train = clean_transcripts['Title_Clean'].astype(str).tolist()\n",
    "all_content_train = clean_transcripts['Title_and_Transcript_Clean'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(all_content_train)]\n",
    "d2v_model = Doc2Vec(tagged_documents, vector_size=100, window=10, min_count=500, workers=7, dm=1, alpha=0.025, min_alpha=0.001)\n",
    "d2v_model.train(tagged_documents, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#https://medium.com/@ermolushka/text-clusterization-using-python-and-doc2vec-8c499668fa61\n",
    "def fit_kmeans(n_clusters):\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=10000)\n",
    "    X = kmeans_model.fit(d2v_model.dv.vectors)\n",
    "    labels = kmeans_model.labels_.tolist()\n",
    "    #l = kmeans_model.fit_predict(d2v_model.dv.vectors) #does nothing?\n",
    "    pca = PCA(n_components=10,random_state=42).fit(d2v_model.dv.vectors)\n",
    "    datapoint = pca.transform(d2v_model.dv.vectors)\n",
    "    value_counts = Counter(labels)\n",
    "    return datapoint, value_counts\n",
    "datapoint, value_counts = fit_kmeans(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary shows the count videos in each kmeans cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 451, 16: 351, 9: 166, 15: 135, 4: 122, 1: 112, 6: 111, 3: 88, 18: 70, 11: 63, 17: 61, 10: 44, 8: 29, 7: 17, 0: 16, 13: 13, 19: 13, 12: 11, 14: 4, 5: 4})\n"
     ]
    }
   ],
   "source": [
    "print(value_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the Clusters with the Least Points which may be more informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cluster Numbers from Least to Most Points [14, 5, 12, 13, 19, 0, 7, 8, 10, 17, 11, 18, 3, 6, 1, 4, 15, 9, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "sorted_keys = sorted(value_counts, key=lambda k: value_counts[k])\n",
    "print(f\"The Cluster Numbers from Least to Most Points {sorted_keys})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is kmeans group 14\n",
      " \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[39mif\u001b[39;00m found \u001b[39m==\u001b[39mtotal_examples:\n\u001b[1;32m     10\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m print_group_examples(sorted_keys[\u001b[39m0\u001b[39;49m],\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mprint_group_examples\u001b[0;34m(group_num, total_examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m found \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m count \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(clean_titles_list)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mif\u001b[39;00m labels[count]\u001b[39m==\u001b[39mgroup_num:\n\u001b[1;32m      7\u001b[0m         \u001b[39mprint\u001b[39m(clean_titles_list[count])\n\u001b[1;32m      8\u001b[0m         found\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "def print_group_examples(group_num, total_examples):\n",
    "    print(f\"Here is kmeans group {group_num}\")\n",
    "    print(\" \")\n",
    "    found = 0\n",
    "    for count in range(len(clean_titles_list)):\n",
    "        if labels[count]==group_num:\n",
    "            print(clean_titles_list[count])\n",
    "            found+=1\n",
    "        if found ==total_examples:\n",
    "            break\n",
    "print_group_examples(sorted_keys[0],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is kmeans group 13\n",
      " \n",
      "Taste Expert Answers Questions From Twitter | Tech Support | WIRED\n",
      "Audiologist Answers Hearing Questions From Twitter | Tech Support | WIRED\n",
      "James Dyson Answers Design Questions From Twitter | Tech Support | WIRED\n",
      "Veterinarian Answers Pet Questions From Twitter | Tech Support | WIRED\n",
      "What Is The Best Position For Descending? | GCN Tech Clinic\n"
     ]
    }
   ],
   "source": [
    "print_group_examples(sorted_keys[1],5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot K Means -- Though manual search above is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plot_kmeans(n_clusters):\n",
    "    try:\n",
    "        fig, ax = plt.subplots()\n",
    "        label1 = [\n",
    "        \"#FF0000\", \"#00FF00\", \"#0000FF\", \"#FFFF00\", \"#00FFFF\", \"#FF00FF\",  # Primary colors and cyan, magenta, and yellow\n",
    "        \"#800000\", \"#008000\", \"#000080\", \"#808000\", \"#008080\", \"#800080\",  # Darker shades of the primary colors and their combinations\n",
    "        \"#FFA500\", \"#FFFF99\", \"#FF99CC\", \"#CC99FF\", \"#99CCFF\", \"#99FFCC\",  # Orange, light yellow, light pink, light purple, light blue-green combinations\n",
    "        \"#FFC0CB\", \"#800080\", \"#008080\", \"#00FF00\", \"#FF0000\", \"#0000FF\",  # Pink, purple, teal, lime, red, blue\n",
    "        \"#000000\", \"#808080\", \"#C0C0C0\", \"#FFFFFF\", \"#F5F5DC\", \"#A52A2A\",  # Black, gray shades, white, beige, brown\n",
    "        \"#F0FFFF\", \"#F0F8FF\", \"#FAEBD7\", \"#7FFFD4\", \"#F5F5F5\", \"#FFD700\",  # Azure, alice blue, antique white, aquamarine, white smoke, gold\n",
    "        \"#D2B48C\", \"#ADFF2F\", \"#FF69B4\", \"#4B0082\", \"#7FFF00\", \"#800000\"]  # Tan, green-yellow, hot pink, indigo, chartreuse, maroon]    label1 = labels1[:n_clusters]\n",
    "        color = [label1[i] for i in labels]\n",
    "\n",
    "        scatter = ax.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "        # Create the cursor object\n",
    "        cursor = mplcursors.cursor(scatter)\n",
    "        # Define the tooltip content\n",
    "        @cursor.connect(\"add\")\n",
    "        def on_add(sel):\n",
    "            index = sel.target.index\n",
    "            label = labels[index]\n",
    "            transcript = clean_titles_list[index]  # Get the transcript based on the index\n",
    "            sel.annotation.set_text(f'Label: {label}\\nTranscript: {transcript}')\n",
    "\n",
    "        # Add labels to each data point\n",
    "        for i, label in enumerate(labels):\n",
    "            ax.text(datapoint[i, 0], datapoint[i, 1], label, ha='center', va='center')\n",
    "\n",
    "        plt.show()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "#plot_kmeans(n_clusters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Closest Cluster of a new Query of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SAT', 'Math', 'Prep', '-', 'Calculator', 'Practice', 'Test', '1']\n",
      "['SAT Math Prep - Calculator Practice Test 1']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'd2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30536\\438947673.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#d2v_model = Doc2Vec(tagged_documents, vector_size=100, window=10, min_count=500, workers=7, dm=1, alpha=0.025, min_alpha=0.001)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#d2v_model.train(tagged_documents, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0minferred_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#datapoint = pca.transform(new_vec)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "query = \"SAT Math Prep - the No Calculator Practice Test 1\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "query = query.split()\n",
    "query = [word for word in query if word.lower() not in stop_words]\n",
    "print(query)\n",
    "\n",
    "query = [' '.join(query)]\n",
    "print(query)\n",
    "#tagged_documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(all_content_train)]\n",
    "#d2v_model = Doc2Vec(tagged_documents, vector_size=100, window=10, min_count=500, workers=7, dm=1, alpha=0.025, min_alpha=0.001)\n",
    "#d2v_model.train(tagged_documents, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)\n",
    "inferred_vectors = d2v_model.infer_vector(query).reshape(1, -1)\n",
    "#datapoint = pca.transform(new_vec)\n",
    "d2v_model.docvecs.add_documents(new_documents, inferred_vectors) \n",
    "#closest_cluster = int(kmeans_model.predict(new_vec))\n",
    "#print(type(closest_cluster),closest_cluster)\n",
    "print_group_examples(sorted_keys[closest_cluster],5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE Experimental Section Below "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is an alternative to Kmeans that may learn more complex grouping shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "dbscan_model = DBSCAN(eps=0.7, min_samples=15)\n",
    "X = dbscan_model.fit(d2v_model.dv.vectors)\n",
    "\n",
    "labels = dbscan_model.labels_.tolist()\n",
    "pca = PCA(n_components=5).fit(d2v_model.dv.vectors)\n",
    "datapoint = pca.transform(d2v_model.dv.vectors)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "label1 = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF', '#00FFFF', '#800080', '#FFA500']\n",
    "color = [label1[i] for i in labels]\n",
    "\n",
    "scatter = ax.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "# Create the cursor object\n",
    "cursor = mplcursors.cursor(scatter)\n",
    "\n",
    "# Define the tooltip content\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    index = sel.target.index\n",
    "    label = labels[index]\n",
    "    transcript = clean_titles_list[index]  # Get the transcript based on the index\n",
    "    sel.annotation.set_text(f'Label: {label}\\nTranscript: {transcript}')\n",
    "\n",
    "# Add labels to each data point\n",
    "for i, label in enumerate(labels):\n",
    "    ax.text(datapoint[i, 0], datapoint[i, 1], label, ha='center', va='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create 30 evenly spaced values between 0.1 and 1.0 for eps\n",
    "eps_values = np.linspace(0.1, 1.0, 30)\n",
    "\n",
    "# Create 30 evenly spaced values between 5 and 50 for min_samples (rounded to nearest integer)\n",
    "min_samples_values = np.round(np.linspace(5, 50, 30)).astype(int)\n",
    "\n",
    "# Combine these into pairs\n",
    "param_pairs = list(zip(eps_values, min_samples_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 clusters with eps=0.1, min_samples=5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# Define the parameter pairs to try\n",
    "#param_pairs = [(0.5, 5), (0.3, 10), (0.7, 3), (0.2, 10), (1.0, 5)]\n",
    "try:\n",
    "    more_found = False\n",
    "    for eps, min_samples in param_pairs:\n",
    "        # Fit DBSCAN model\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        X = dbscan_model.fit(d2v_model.dv.vectors)\n",
    "\n",
    "        # Get labels\n",
    "        labels = dbscan_model.labels_.tolist()\n",
    "\n",
    "        # Check if we have more than one cluster (excluding noise, represented by -1)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters > 1:\n",
    "            more_found = True\n",
    "            print(f\"Found {n_clusters} clusters with eps={eps}, min_samples={min_samples}\")\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    if not more_found:\n",
    "        print(\"only one cluster found\")\n",
    "    # Continue with the script if more than one cluster found\n",
    "    pca = PCA(n_components=2).fit(d2v_model.dv.vectors)\n",
    "    datapoint = pca.transform(d2v_model.dv.vectors)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    label1 = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF', '#00FFFF', '#800080', '#FFA500']\n",
    "    color = [label1[i] for i in labels]\n",
    "\n",
    "    scatter = ax.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "    # Create the cursor object\n",
    "    cursor = mplcursors.cursor(scatter)\n",
    "\n",
    "    # Define the tooltip content\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        index = sel.target.index\n",
    "        label = labels[index]\n",
    "        transcript = clean_titles_list[index]  # Get the transcript based on the index\n",
    "        sel.annotation.set_text(f'Label: {label}\\nTranscript: {transcript}')\n",
    "\n",
    "    # Add labels to each data point\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.text(datapoint[i, 0], datapoint[i, 1], label, ha='center', va='center')\n",
    "\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29e8d7c574c1c3fa7d750bb72c4ea3c797abd9d323046819b3334e09977e0d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
